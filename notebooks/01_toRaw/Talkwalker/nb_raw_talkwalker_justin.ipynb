{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d33bab4c-f97e-498f-83f8-0f18dbdf42c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# 01-raw: Auslesen der Volltexte aus Talkwalker\n",
    "\n",
    "* Um was handelt es sich hier  (Kurzbeschreibung Inhalt):  \n",
    "Anbinden der Talkwalker API um Social Listening Daten in 01_Raw abzuspeichern.\n",
    "\n",
    "\n",
    "---\n",
    "## QUELLEN:  \n",
    "- Talkwalker API \n",
    "\n",
    "## ZIEL  \n",
    "- Unity-Catalog: \n",
    "  - datif_pz_uk_dev.01-toRaw.talkwalker\n",
    "\n",
    "  \n",
    "---\n",
    "* Versionen (aktuelle immer oben):\n",
    "- 10.07.2025 Max Mustermann: Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31f5c6ba-7fdf-4169-b61f-81cddf9236d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../common/nb_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a37ea230-df9b-45e4-8909-06a6ab810157",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DoubleType,\n",
    "    BooleanType, TimestampType, MapType, ArrayType, DateType  \n",
    ")\n",
    "import datetime\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac5165c2-e6a8-4591-a864-fc799c1d550d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema name and path to store tables\n",
    "target_schema_name = \"02_cleaned\"\n",
    "target_path = \"talkwalker_raw\"\n",
    "target_table_name=\"talkwalker_beispiel_30d\"\n",
    "# Set source and trg path\n",
    "source_path = sta_endpoint_pz_uk[\"01_raw\"] + \"/talkwalker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d09578d1-181b-4520-b08e-44acf821fa68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "secret= get_secret(\"Talkwalker-API\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38950486-c4da-4da8-9bc9-baea26cd0cbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get(URL: str) -> dict:\n",
    "    \"\"\"\n",
    "    Führt eine GET-Anfrage an die angegebene URL mit dem Talkwalker-API-Schlüssel als Autorisierungsschlüssel aus und gibt das Ergebnis als JSON-Daten zurück.\n",
    "\n",
    "    Input:\n",
    "    URL: URL Endpoint\n",
    "\n",
    "    Output:\n",
    "    result: JSON-Daten\n",
    "    \"\"\"\n",
    "    response = requests.get(URL, headers={\"Authorization\": f\"Bearer {secret}\"})\n",
    "    if response.status_code == 200:\n",
    "        print(\"✅ API funktioniert!\")\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"❌ Fehler beim API-Aufruf:\", response.status_code)\n",
    "        print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3c8acca-9b00-4f85-a264-1b9683ec18f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37079f22-99c7-411b-90ac-6f361116c4e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "status_url = \"https://api.talkwalker.com/api/v1/status/credits\"\n",
    "data = get(status_url)\n",
    "print(data)\n",
    "print(\"Verbleibende monatliche Credits:\", data[\"result_creditinfo\"][\"remaining_credits_monthly\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49ad81d3-2807-497e-9651-12506b38cea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get Search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61f8c77c-1de5-4de4-b281-634240859a51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "projekt_url = \"https://api.talkwalker.com/api/v1/search/info\"\n",
    "data = get(projekt_url)\n",
    "print(data)\n",
    "for project in data[\"result_accinfo\"][\"projects\"]:\n",
    "    print(\"Projekt:\", project[\"name\"], \"| ID:\", project[\"id\"])\n",
    "project_id = data[\"result_accinfo\"][\"projects\"][0][\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cc60e9f-3ddc-47fb-9a50-b166c76a337e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a2504f-0a7c-4ac4-844f-ba29b2d8331c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "topic_result_url = f\"https://api.talkwalker.com/api/v2/talkwalker/p/{project_id}/topics/list\" \n",
    "\n",
    "data = get(topic_result_url)\n",
    "\n",
    "# print(data)\n",
    "sub_topic_dict = {}\n",
    "for topic in data[\"result_topics\"][\"topic_categories\"]:\n",
    "    for sub_topic in topic[\"query_topics\"]:\n",
    "        if topic[\"title\"] == \"ICC UK\":\n",
    "            print(f\"Topic: {topic[\"title\"]}, ID: {topic[\"id\"]}, Subtopic: {sub_topic['title']}, ID: {sub_topic['id']}\")\n",
    "            sub_topic_dict[sub_topic[\"title\"]] = sub_topic[\"id\"]\n",
    "            topic_id = topic[\"id\"]\n",
    "\n",
    "print(topic_id)\n",
    "print(sub_topic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a7ff664-a67a-462f-8308-e8238052e282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Get Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3cf9708-0cee-458a-8b32-4dc0eb05895e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# summary_result_url = f\"https://api.talkwalker.com/api/v1/search/p/{project_id}/summary?time_range=1d&topic={topic_id}&q=A%20AND%20A\"\n",
    "\n",
    "# data = get(summary_result_url)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4db7367b-e45a-4fed-b6ef-55adeab032b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get Project Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "360abda5-63cf-4a9a-8186-60a9a4b958f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Liste zum speichern der Daten\n",
    "df_list = []\n",
    "\n",
    "# Set von allen Keys bzw Spalten Namen\n",
    "all_columns = set()\n",
    "\n",
    "for subtopic_title, subtopic_id in sub_topic_dict.items():\n",
    "    url = f\"https://api.talkwalker.com/api/v1/search/p/{project_id}/results?time_range=30d&topic={subtopic_id}&timezone=Europe/Berlin&hpp=500\"\n",
    "    data = get(url)\n",
    "    \n",
    "    try:\n",
    "        for item in data[\"result_content\"][\"data\"]:\n",
    "            flat = item.get(\"data\")\n",
    "            all_columns.update(data.keys())\n",
    "            if flat and isinstance(flat, dict) and len(flat) > 0:\n",
    "                flat[\"subtopic\"] = subtopic_title\n",
    "                df_list.append(flat)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing item: {e}\")\n",
    "        continue\n",
    "\n",
    "# Daten in ein gemeinsames DataFrame überführen (mit automatischer Spaltenangleichung)\n",
    "df = pd.DataFrame(df_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc451a8a-3bce-421b-8602-f48103a9e390",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Convert Publishe Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1105c96f-14bd-41dd-9477-3aa8e291b706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Timestamp konvertieren\n",
    "df[\"published\"] = pd.to_numeric(df[\"published\"], errors=\"coerce\")\n",
    "df[\"published_dt\"] = pd.to_datetime(df[\"published\"], unit=\"ms\", utc=True)\n",
    "df[\"published_dt_local\"] = df[\"published_dt\"].dt.tz_convert(\"Europe/Berlin\")\n",
    "df[\"published_date\"] = pd.to_datetime(df[\"published\"], unit=\"ms\", errors=\"coerce\").dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41553c47-c4bb-485e-acc6-486e75a8f7c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac636490-5cef-4229-ae6c-8ae0ad2dfeb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec808587-f43c-4e05-8d74-6ead042d9e3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eedb77f2-933d-4a35-9b47-263db64dced3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def infer_type(val: object) -> object:\n",
    "    \"\"\"\n",
    "    Infers the corresponding PySpark data type for a given Python or pandas value.\n",
    "\n",
    "    Args:\n",
    "        val (object): A single value from a pandas DataFrame column. This can be a primitive type\n",
    "                      (int, float, str, bool), a datetime/date, list, dict, or a pandas-specific type.\n",
    "\n",
    "    Returns:\n",
    "        object: A PySpark data type (e.g., StringType, IntegerType, DoubleType, etc.)\n",
    "                suitable for use in a StructField.\n",
    "    \"\"\"\n",
    "    if isinstance(val, (bool, np.bool_)):\n",
    "        return BooleanType()\n",
    "    elif isinstance(val, (int, np.integer)):\n",
    "        # Spark IntegerType supports only 32-bit integers\n",
    "        if val > 2_147_483_647:\n",
    "            return StringType()\n",
    "        return IntegerType()\n",
    "    elif isinstance(val, (float, np.floating)):\n",
    "        return DoubleType()\n",
    "    elif isinstance(val, pd.Timestamp):\n",
    "        return TimestampType()\n",
    "    elif isinstance(val, (pd.Timestamp, datetime.datetime)):\n",
    "        return TimestampType()\n",
    "    elif isinstance(val, datetime.date):\n",
    "        return DateType()\n",
    "    elif isinstance(val, dict):\n",
    "        return MapType(StringType(), StringType(), True)\n",
    "    elif isinstance(val, list):\n",
    "        return ArrayType(StringType(), True)\n",
    "    else:\n",
    "        return StringType()\n",
    "\n",
    "\n",
    "def infer_spark_schema_from_pandas(df: pd.DataFrame) -> StructType:\n",
    "    \"\"\"\n",
    "    Infers a complete PySpark StructType schema from a pandas DataFrame\n",
    "    by inspecting the first non-null value in each column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input pandas DataFrame whose schema is to be inferred.\n",
    "\n",
    "    Returns:\n",
    "        StructType: A PySpark StructType object representing the schema of the DataFrame.\n",
    "    \"\"\"\n",
    "    fields = []\n",
    "    for col in df.columns:\n",
    "        # Use the first non-null sample value in the column\n",
    "        sample_value = df[col].dropna().iloc[0] if df[col].dropna().size > 0 else None\n",
    "        # print(f\"{col}: {sample_value}, {type(sample_value)}\")\n",
    "        spark_type = infer_type(sample_value)\n",
    "        fields.append(StructField(col, spark_type, True))\n",
    "\n",
    "    return StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f12264-02f6-42f3-878b-92a95d784946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalize_pandas_types(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalisiert die Datentypen eines pandas DataFrames, sodass sie für\n",
    "    eine Konvertierung nach PySpark möglichst kompatibel und robust sind.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Ursprünglicher pandas DataFrame mit möglicherweise uneinheitlichen Typen.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Neuer DataFrame mit bereinigten und standardisierten Datentypen.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        non_na = df_clean[col].dropna()\n",
    "        if non_na.empty:\n",
    "            continue  # keine Werte → egal\n",
    "\n",
    "        sample = non_na.iloc[0]\n",
    "\n",
    "        if isinstance(sample, (int, np.integer)):\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors=\"coerce\").astype(\"Int64\")\n",
    "        elif isinstance(sample, (float, np.floating)):\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors=\"coerce\").astype(float)\n",
    "        elif isinstance(sample, (str, bool, pd.Timestamp)):\n",
    "            pass  # keine Änderung nötig\n",
    "        elif isinstance(sample, datetime.date):\n",
    "            df_clean[col] = df_clean[col].apply(lambda x: x if pd.notnull(x) else None)\n",
    "        elif isinstance(sample, list):\n",
    "            df_clean[col] = df_clean[col].apply(lambda x: x if isinstance(x, list) else [])\n",
    "        elif isinstance(sample, dict):\n",
    "            df_clean[col] = df_clean[col].apply(lambda x: x if isinstance(x, dict) else {})\n",
    "        else:\n",
    "            df_clean[col] = df_clean[col].astype(str)\n",
    "\n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69718c22-787b-4594-8a40-ad7ff4df1d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_norm = normalize_pandas_types(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a184674a-f479-45fc-b765-ad8ceeef78b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Schema = infer_spark_schema_from_pandas(df_norm)\n",
    "# print(Schema)\n",
    "df_final = spark.createDataFrame(df_norm, schema=Schema)\n",
    "\n",
    "\n",
    "\n",
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afd376fd-7a03-4c02-be26-6232d5669d71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fn_overwrite_table(\n",
    "    df_source=df_final,\n",
    "    target_schema_name=target_schema_name,\n",
    "    target_table_name=target_table_name,\n",
    "    target_path=target_path\n",
    ")\n",
    "print(\"✅ Tabelle erfolgreich aktualisiert.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_raw_talkwalker_justin",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
