{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d33bab4c-f97e-498f-83f8-0f18dbdf42c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# 01-raw: Auslesen der Volltexte aus Talkwalker\n",
    "\n",
    "* Um was handelt es sich hier  (Kurzbeschreibung Inhalt):  \n",
    "Anbinden der PMG API um Print und Online Medien Daten in 01_Raw abzuspeichern.\n",
    "\n",
    "\n",
    "---\n",
    "## QUELLEN:  \n",
    "- PMG API\n",
    "\n",
    "## ZIEL  \n",
    "- Unity-Catalog: \n",
    "  - datif_pz_uk_dev.01-toRaw.pmg\n",
    "\n",
    "  \n",
    "---\n",
    "* Versionen (aktuelle immer oben):\n",
    "- 06.08.2025 Max Mustermann: Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31f5c6ba-7fdf-4169-b61f-81cddf9236d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../common/nb_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a37ea230-df9b-45e4-8909-06a6ab810157",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DoubleType,\n",
    "    BooleanType, TimestampType, MapType, ArrayType, DateType  \n",
    ")\n",
    "import datetime\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import zoneinfo \n",
    "import pytz\n",
    "import xml.etree.ElementTree as ET\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "import json\n",
    "from azure.storage.blob import BlobServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac5165c2-e6a8-4591-a864-fc799c1d550d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pz_storage_name = get_secret(\"storage-datalake-name\")\n",
    "pz_storage_key = get_secret(\"storage-access-key\")\n",
    "connection_string = f'DefaultEndpointsProtocol=https;AccountName={pz_storage_name};AccountKey={pz_storage_key};EndpointSuffix=core.windows.net'\n",
    "json_base_path = f\"abfss://01-raw@{pz_storage_name}.dfs.core.windows.net/pmg/json/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d09578d1-181b-4520-b08e-44acf821fa68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "secret= get_secret(\"pmg-api\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeebfa73-b7cb-45e2-85d1-046e77516107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Funktion um die APIs abzurufen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e053949-e915-48c4-a2b0-47543012d7fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get(URL: str,) -> dict:\n",
    "    \"\"\"\n",
    "    Führt eine GET-Anfrage an die angegebene URL mit dem API-Key im Header 'auth' aus.\n",
    "\n",
    "    Input:\n",
    "    - api_key: Der API-Schlüssel für den Zugriff\n",
    "\n",
    "    Output:\n",
    "    - JSON-Antwort als dict\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"auth\": secret\n",
    "    }\n",
    "\n",
    "    response = requests.get(URL, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(\" API funktioniert!\")\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"Fehler beim API-Aufruf:\", response.status_code)\n",
    "        print(response.text)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e22b2a4-4e2a-4b2d-8be6-62639e2b7967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def get_xml(url: str) -> ET.Element:\n",
    "    \"\"\"\n",
    "    Funktion um den Content als xml Datei zu laden.\n",
    "\n",
    "    Input\n",
    "    -url: URL der XML-Datei\n",
    "\n",
    "    Output:\n",
    "    - Element-Tree-Objekt der XML-Datei\n",
    "    \"\"\"\n",
    "    \n",
    "    headers={\"auth\": secret}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        #print(\"XML erfolgreich geladen.\")\n",
    "        try:\n",
    "            root = ET.fromstring(response.content)\n",
    "            return root\n",
    "        except ET.ParseError as e:\n",
    "            print(\"Fehler beim XML-Parsen:\", e)\n",
    "    else:\n",
    "        print(\"Fehler beim API-Aufruf:\", response.status_code)\n",
    "        print(response.text)\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c8acca-9b00-4f85-a264-1b9683ec18f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Name der gespeicherten Suche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fbb712e-39b6-48ce-bceb-e24a4daa79b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_first_feed_url(stored_search_url: str = \"https://api-mediahub.presse-monitor.de/neo/api/v1/storedsearch\") -> dict:\n",
    "    \"\"\"\n",
    "    Fetches the feed URL from the stored search endpoint for every search as dict.\n",
    "\n",
    "    This function calls the given stored search API endpoint, retrieves the JSON data,\n",
    "    and extracts the `href` value of the first `_links` entry from the first `items` element.\n",
    "\n",
    "    Args:\n",
    "        stored_search_url (str): The API endpoint URL for stored searches.\n",
    "\n",
    "    Returns:\n",
    "        str: dict with the feed URL for every search\n",
    "\n",
    "    \"\"\"\n",
    "    dict = {}\n",
    "    data = get(stored_search_url)\n",
    "    for i in data[\"items\"]:\n",
    "        dict[i[\"name\"]] = i[\"_links\"][0][\"href\"]\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5939f0a4-286b-4f91-810f-e8e010d3e212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "908148e5-6319-4e6f-bb02-7494487d633c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Daten aus der Trefferliste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38cf0b27-c5bb-4828-b586-af3d75820cfb",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"version\":331,\"home_page_url\":246,\"feed_url\":423,\"_new_url\":522},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"version\":{\"format\":{\"preset\":\"string-preset-url\"}},\"home_page_url\":{\"format\":{\"preset\":\"string-preset-url\"}},\"feed_url\":{\"format\":{\"preset\":\"string-preset-url\"}},\"_new_url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1754561119290}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_data(feed_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Führt eine GET-Anfrage an die angegebene URL mit dem API-Key im Header 'auth' aus.\n",
    "\n",
    "    Input:\n",
    "    - feed_url: Url Endpoint für den Zugriff\n",
    "\n",
    "    Output:\n",
    "    - pd.DataFrame mit der Trefferliste\n",
    "    \"\"\"\n",
    "\n",
    "    # try:\n",
    "    #     data = pd.DataFrame(get(feed_url))\n",
    "    #     return data\n",
    "    \n",
    "    # except ValueError as ve:\n",
    "    #             if \"If using all scalar values, you must pass an index\" in str(ve):\n",
    "    #                 print(f\"URL gibt keine neuen Daten zurück: {ve}\")\n",
    "    #                 return None  # Kein Fallback\n",
    "    return pd.DataFrame(get(feed_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1347efba-39c9-4ba1-83bc-8a9255cc1640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Inhalt aus Index als seperate Spalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34820224-dacc-4c9e-824a-29fc8ce71263",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"version\":{\"format\":{\"preset\":\"string-preset-url\"}},\"home_page_url\":{\"format\":{\"preset\":\"string-preset-url\"}},\"feed_url\":{\"format\":{\"preset\":\"string-preset-url\"}},\"_new_url\":{\"format\":{\"preset\":\"string-preset-url\"}},\"items_id\":{\"format\":{\"preset\":\"string-preset-url\"}},\"items_url\":{\"format\":{\"preset\":\"string-preset-url\"}},\"items_next_url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1754908102409}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_index_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Fügt das Dict aus der Spalte Items als seperate Spalten hinzu\n",
    "\n",
    "  Input:\n",
    "  - data: DataFrame mit der Trefferliste\n",
    "\n",
    "  Output:\n",
    "  - pd.DataFrame mit zusätzlich den Items als Spalten\n",
    "  \"\"\"\n",
    "    \n",
    "  items = data[\"items\"].apply(pd.Series)\n",
    "  items = items.add_prefix(\"items_\")\n",
    "\n",
    "  return pd.concat([pd.DataFrame(data).drop(\"items\", axis=1), items], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "652edb3d-4dad-4423-ab5d-a47b7f198ffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Inhalt aus Attachment als seperate Spalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df339007-b744-4660-8d69-632da94ab1e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_attachment_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fügt das Dict aus der Spalte Attachment als seperate Spalten hinzu\n",
    "\n",
    "    Input:\n",
    "    - data: DataFrame mit der Trefferliste\n",
    "\n",
    "    Output:\n",
    "    - pd.DataFrame mit zusätzlich den Items als Spalten\n",
    "    \"\"\"\n",
    "\n",
    "    attachments_df = data[\"items_attachments\"].apply(\n",
    "        lambda x: x[0] if isinstance(x, list) and x else {}\n",
    "    ).apply(pd.Series)\n",
    "    attachments_df = attachments_df.add_prefix(\"attachment_\")\n",
    "\n",
    "    df_with_items_attachments = pd.concat([data.drop(\"items_attachments\", axis=1), attachments_df], axis=1)\n",
    "\n",
    "    return pd.concat([data.drop(\"items_attachments\", axis=1), attachments_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb19a0b3-22c1-4721-bac6-c6d870a78f40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Timestamp hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07737a9c-2a09-40e1-a3d4-a82b98e23085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_timestamp(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fügt dem DataFrame eine Spalte mit dem Timestamp hinzu\n",
    "\n",
    "    Input:\n",
    "    - df: DataFrame mit der Trefferliste\n",
    "\n",
    "    Output:\n",
    "    - pd.DataFrame mit zusätzlich der Spalte timestamp\n",
    "    \"\"\"\n",
    "    berlin_tz = pytz.timezone(\"Europe/Berlin\")\n",
    "    df[\"timestamp\"] = datetime.now(berlin_tz).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3186f46-9847-4a53-a552-c32784e61f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Content als serperate Spalte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b5f55ea-2566-48fc-aad5-cf7a03476662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetch_content_xml(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Fetches XML content from a list of content URLs in a DataFrame and returns it as a Series.\n",
    "\n",
    "    This function iterates over each URL in the specified DataFrame column, \n",
    "    retrieves the XML from the API via `get_xml()`, converts it to a Unicode string, \n",
    "    and stores it in a Pandas Series. The Series retains the original index of the input DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing the URLs.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Pandas Series named `content_xml` with the same index as `df`, \n",
    "                   where each element is the XML content as a string.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for url in df[\"items_url\"]:\n",
    "        # Get XML Element from API\n",
    "        xml_element = get_xml(url)\n",
    "        # Convert XML Element to string\n",
    "        xml_str = ET.tostring(xml_element, encoding=\"unicode\")\n",
    "        if isinstance(xml_str, (list, tuple)):\n",
    "            xml_str = xml_str[0] if xml_str else None\n",
    "        content.append(xml_str)\n",
    "    df_content = pd.Series(content, name=\"content_xml\", index=df.index)\n",
    "\n",
    "    return pd.concat([df_with_items_attachments, df_content], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1a89bac-9285-4748-bc91-da8ffb88a7a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def expand_xml_column(df: pd.DataFrame, xml_column: str = \"content_xml\", prefix: str = \"content_\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expands an XML string column in a DataFrame into separate columns for each unique tag path,\n",
    "    including nested elements, attributes, and repeated tags (with index suffix).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing the XML string column.\n",
    "        xml_column (str): Name of the column containing XML strings.\n",
    "        prefix (str): Prefix to prepend to all generated column names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with additional columns for each flattened XML field.\n",
    "    \"\"\"\n",
    "\n",
    "    def flatten_xml(elem, parent_path=\"\"):\n",
    "        \"\"\"\n",
    "        Recursively flattens an XML element into a dictionary of key-value pairs with full paths.\n",
    "        Repeated tags get a numeric suffix (.0, .1, ...).\n",
    "        Attributes are stored as path@attr=value.\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        path = f\"{parent_path}.{elem.tag}\" if parent_path else elem.tag\n",
    "\n",
    "        # Add attributes as separate keys\n",
    "        for attr, val in elem.attrib.items():\n",
    "            data[f\"{path}@{attr}\"] = val\n",
    "\n",
    "        # Add element text, if meaningful\n",
    "        text = elem.text.strip() if elem.text and elem.text.strip() else None\n",
    "        if text:\n",
    "            data[path] = text\n",
    "\n",
    "        # Count child tags to track duplicates\n",
    "        tag_counts = {}\n",
    "        for child in elem:\n",
    "            tag_counts[child.tag] = tag_counts.get(child.tag, 0) + 1\n",
    "\n",
    "        # Process child elements\n",
    "        child_index = {}\n",
    "        for child in elem:\n",
    "            tag = child.tag\n",
    "            count = tag_counts[tag]\n",
    "\n",
    "            if count > 1:\n",
    "                idx = child_index.get(tag, 0)\n",
    "                child_path = f\"{path}.{tag}.{idx}\"\n",
    "                child_index[tag] = idx + 1\n",
    "            else:\n",
    "                child_path = f\"{path}.{tag}\"\n",
    "\n",
    "            data.update(flatten_xml(child, parent_path=path))\n",
    "\n",
    "        return data\n",
    "\n",
    "    records = []\n",
    "    for xml_str in df[xml_column]:\n",
    "        try:\n",
    "            root = ET.fromstring(xml_str)\n",
    "            flattened = flatten_xml(root)\n",
    "        except ET.ParseError:\n",
    "            flattened = {}\n",
    "        records.append(flattened)\n",
    "\n",
    "    df_flat = pd.DataFrame(records, index=df.index).add_prefix(prefix)\n",
    "    return pd.concat([df, df_flat], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d4bfd98-1c4f-46c1-8dae-4b1009333113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Speicher als .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed2677ea-a344-42cb-a707-0de558c296c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upload_df_ndjson_to_blob(\n",
    "    df: pd.DataFrame,\n",
    "    connection_string: str,\n",
    "    container: str,\n",
    "    blob_path: str,\n",
    ") -> None:\n",
    "    \"\"\"Uploads a pandas DataFrame to Azure Blob Storage as newline-delimited JSON (NDJSON).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to upload.\n",
    "        connection_string (str): Azure Storage connection string.\n",
    "        container (str): Container name, e.g. \"01-raw\".\n",
    "        blob_path (str): Blob path within the container, e.g. \"mediahub/df_master.json\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Pandas -> NDJSON\n",
    "    # Orient=records erzeugt eine Liste von Dicts; wir packen pro Zeile ein JSON-Objekt.\n",
    "    records = df.to_dict(orient=\"records\")\n",
    "    json_lines = \"\\n\".join(json.dumps(r, ensure_ascii=False, default=str) for r in records)\n",
    "\n",
    "    # Upload\n",
    "    bsc = BlobServiceClient.from_connection_string(connection_string)\n",
    "    blob_client = bsc.get_blob_client(container=container, blob=blob_path)\n",
    "    blob_client.upload_blob(json_lines, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4acdcad-90df-4115-8a02-34737993cee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### String pfadsicher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bcbb00b-b630-4f49-a689-a08c3fef3f98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def sanitize_for_path(s: str) -> str:\n",
    "    \"\"\"Macht einen String pfadsicher (Azure/Unix): nur a-zA-Z0-9._-/ ersatzweise '_'.\"\"\"\n",
    "    return re.sub(r\"[^a-zA-Z0-9._\\-]\", \"_\", s.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d38adaf0-8931-4fa3-95e2-771ede116069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Alle Seiten abrufen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45fd82fa-952b-4c47-9b00-cf4b2df98b4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_all_data_as_df(start_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Holt alle Seiten über `next_url` und gibt einen zusammengefügten DataFrame zurück.\n",
    "\n",
    "    Args:\n",
    "        start_df (pd.DataFrame): DataFrame mit den Inhalten der ersten Seite (inkl. 'next_url').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Zusammengefügter DataFrame aller Seiteninhalte.\n",
    "    \"\"\"\n",
    "    all_dfs = [start_df]\n",
    "    \n",
    "    next_url = start_df.loc[0, \"next_url\"] if \"next_url\" in start_df.columns else None\n",
    "\n",
    "    while next_url:\n",
    "        print(f\"Fetching next page: {next_url}\")\n",
    "        try:\n",
    "            next_data = get_data(next_url)\n",
    "            all_dfs.append(next_data)\n",
    "            next_url = next_data.loc[0, \"next_url\"] if \"next_url\" in next_data.columns else None\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Laden von next_url ({next_url}): {e}\")\n",
    "            break\n",
    "    if len(all_dfs) > 1:\n",
    "        print(f\"Found {len(all_dfs)} pages.\")\n",
    "    else:\n",
    "        print(f\"Found only one page.\")\n",
    "    # combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    # combined_df = combined_df.reset_index(drop=True)\n",
    "    return pd.concat(all_dfs, ignore_index=True).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eec9fe03-9026-431b-b526-0caf414bc3f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Start Url bestimmen (feed oder _new url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd500e01-16a8-4ee9-9f57-1a5222770960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_data_from_feed(name: str, feed_url: str, json_base_path: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Gibt API-Daten zurück – bevorzugt _new_url, falls vorhanden.\n",
    "    Wenn _new_url leer ist (spezifischer ValueError), wird None zurückgegeben.\n",
    "    Bei anderen Fehlern wird auf feed_url zurückgegriffen.\n",
    "\n",
    "    Args:\n",
    "        name (str): Name der Suche.\n",
    "        feed_url (str): Ursprüngliche Feed-URL.\n",
    "        json_base_path (str): Pfad zum JSON-Basisordner.\n",
    "\n",
    "    Returns:\n",
    "        dict | None: API-Daten von _new_url oder feed_url, oder None wenn _new_url leer war.\n",
    "    \"\"\"\n",
    "    print(f\"Suche für: {name}\")\n",
    "\n",
    "    try:\n",
    "        latest_df = load_json(f\"{json_base_path}{name}/\", newest_first=True)[0]\n",
    "        new_url = latest_df.loc[0, \"_new_url\"]\n",
    "\n",
    "        if pd.notna(new_url) and new_url != \"\":\n",
    "            print(f\"🔗 Versuche _new_url: {new_url}\")\n",
    "            try:\n",
    "                data = get_data(new_url)\n",
    "                # Provoziert ValueError bei leeren Daten\n",
    "                print(\"_new_url erfolgreich verwendet\")\n",
    "                return data\n",
    "\n",
    "            except ValueError as ve:\n",
    "                if \"If using all scalar values, you must pass an index\" in str(ve):\n",
    "                    print(f\"_new_url gibt keine neuen Daten zurück: {ve}\")\n",
    "                    return None  # Kein Fallback\n",
    "                else:\n",
    "                    print(f\"Unerwarteter ValueError bei _new_url: {ve}\")\n",
    "                    print(\"Fallback auf feed_url\")\n",
    "                    return get_data(feed_url)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler bei _new_url: {e}\")\n",
    "                print(\"Fallback auf feed_url\")\n",
    "                return get_data(feed_url)\n",
    "\n",
    "        else:\n",
    "            print(\"Keine _new_url vorhanden, verwende feed_url\")\n",
    "            return get_data(feed_url)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden der gespeicherten _new_url: {e}\")\n",
    "        print(\"Fallback auf feed_url\")\n",
    "        try:\n",
    "                data = get_data(feed_url)\n",
    "                # Provoziert ValueError bei leeren Daten\n",
    "                print(\"_new_url erfolgreich verwendet\")\n",
    "                return data\n",
    "\n",
    "        except ValueError as ve:\n",
    "                if \"If using all scalar values, you must pass an index\" in str(ve):\n",
    "                    print(f\"feed_url gibt keine neuen Daten zurück: {ve}\")\n",
    "                    return None  # Kein Fallback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a72bc7e-957c-4d50-bdf8-22c1abc94add",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### json laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f276932-519c-4523-8524-c068dcb56a71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_json(base_path: str, recursive: bool = True, newest_first: bool = False) -> list[pd.DataFrame]:\n",
    "    \"\"\"Load all JSON (NDJSON) files from an abfss path into a list of pandas DataFrames,\n",
    "    sorted by file modification time.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): abfss folder path ending with '/', e.g. \".../pmg/json/\".\n",
    "        recursive (bool, optional): If True, also load JSONs from direct subfolders. Defaults to True.\n",
    "        newest_first (bool, optional): If True, return newest → oldest. Otherwise oldest → newest. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list[pd.DataFrame]: List of DataFrames, sorted by file time as specified.\n",
    "    \"\"\"\n",
    "    def _ls(path: str):\n",
    "        try:\n",
    "            return dbutils.fs.ls(path)\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    # 1) Dateien einsammeln\n",
    "    entries = []\n",
    "    for e in _ls(base_path):\n",
    "        if e.name.endswith(\".json\"):\n",
    "            entries.append(e)\n",
    "        elif recursive and e.path.endswith(\"/\"):\n",
    "            for f in _ls(e.path):\n",
    "                if f.name.endswith(\".json\"):\n",
    "                    entries.append(f)\n",
    "\n",
    "    if not entries:\n",
    "        print(\"Keine JSON-Dateien gefunden.\")\n",
    "        return []\n",
    "\n",
    "    # 2) Sortieren nach Änderungszeit (Fallback: Pfadname)\n",
    "    # Databricks FileInfo hat i.d.R. 'modificationTime'\n",
    "    def sort_key(fi):\n",
    "        mt = getattr(fi, \"modificationTime\", None)\n",
    "        return (mt if mt is not None else 0, fi.path)\n",
    "\n",
    "    entries.sort(key=sort_key)  # älteste → neueste\n",
    "    if newest_first:\n",
    "        entries.reverse()\n",
    "\n",
    "    # 3) Einlesen\n",
    "    df_list = []\n",
    "    for fi in entries:\n",
    "        sdf = spark.read.json(fi.path)\n",
    "        df_list.append(sdf.toPandas())\n",
    "\n",
    "    print(f\"Es wurden {len(df_list)} JSON-Dateien eingelesen. Reihenfolge: \"\n",
    "          f\"{'neueste → älteste' if newest_first else 'älteste → neueste'}.\")\n",
    "\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c18db2d7-0d66-4481-87ba-0cae8ed2bc10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ausführung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6bf33f8-3515-4f0a-853f-20dee419f1c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get feed_url from the search\n",
    "feed_url_dict = get_first_feed_url()\n",
    "\n",
    "for name, url in feed_url_dict.items():\n",
    "    print(f\"Suche {name} mit URL: {url}\")\n",
    "    base_path = f\"abfss://01-raw@{pz_storage_name}.dfs.core.windows.net/pmg/json/{name}\"\n",
    "    # get data from start url\n",
    "    data = get_latest_data_from_feed(name, url, json_base_path)\n",
    "    # Check ob es keine neuen Daten gibt\n",
    "    if data is None:\n",
    "        print(f\"Keine neuen Daten für {name}, überspringe.\")\n",
    "        continue\n",
    "    \n",
    "    # get next_url data if present\n",
    "    data_next_url = get_all_data_as_df(data)\n",
    "    # get items as separate columns\n",
    "    df_with_items = get_index_data(data_next_url)\n",
    "    # get attachments as separate columns\n",
    "    df_with_items_attachments = get_attachment_data(df_with_items)\n",
    "    # add timestamp\n",
    "    df_timestamp = add_timestamp(df_with_items_attachments)\n",
    "    # get context as separate columns\n",
    "    df_content = fetch_content_xml(df_timestamp)\n",
    "    df_master = expand_xml_column(df_content)\n",
    "\n",
    "    file_name = sanitize_for_path(f\"abzug_{df_master.loc[0, 'timestamp']}.json\")\n",
    "    # save as json\n",
    "    upload_df_ndjson_to_blob(\n",
    "        df=df_master,\n",
    "        connection_string=connection_string,  # better via Databricks Secrets\n",
    "        container=\"01-raw\",\n",
    "        blob_path=f\"pmg/json/{name}/{file_name}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_raw_pmg",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
